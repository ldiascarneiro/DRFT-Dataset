# DRFT: Draftâ€“Reviewâ€“Final Triples Dataset

[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)
[![Paper](https://img.shields.io/badge/Paper-PDF-blue)](link_para_o_paper_se_disponivel)

## ğŸ“Œ Overview
**DRFT (Draftâ€“Reviewâ€“Final Triples)** is a curated dataset of **225 triplets** that capture the **peer-review cycle** in scientific publishing.  
Each instance contains:
- The **initial draft** (submission)
- The **peer reviews** (concatenated comments and/or meta-review)
- The **final published version** (arXiv)

The dataset is distributed as a single `.xlsx` file with standardized schema and evaluation signals.

![Dataset Pipeline](figures/FlowChart.png)

---

## ğŸ“Š Key Features
- **225 aligned draftâ€“reviewâ€“final triplets** (ICLR + PeerRead).
- **Two automatic metrics** included:
  - **BERTScore** (draft â†” final) â€“ semantic preservation
  - **G-Eval** (review â†’ final, GPT-5o-mini) â€“ review uptake with justification
- **Compact, high-value benchmark** for:
  - Review-conditioned edit planning
  - Evaluating automatic revisions vs. human finals
  - Analyzing review facets and revision magnitude

---

## ğŸ“‚ Repository Structure
![Dataset Pipeline](figures/DRFT_github_struct.png)



---

## ğŸ“ Schema (Fields)
Each row in the `.xlsx` dataset contains:

- **base_text** â†’ Full text of the initial submission  
- **review** â†’ Concatenated peer-review comments/meta-review  
- **final_text** â†’ Final version (arXiv)  
- **source** â†’ Data source (ICLR/OpenReview, PeerRead)  
- **bertscore** â†’ Semantic similarity score (draft â†” final)  
- **g_eval_score** â†’ Review uptake score (1â€“10)  
- **g_eval_justification** â†’ Short justification generated by GPT-5o-mini  

---

## ğŸ“ˆ Corpus Statistics
- **Base text length (tokens):** mean â‰ˆ 4700  
- **Final text length (tokens):** mean â‰ˆ 4600  
- **BERTScore:** mean â‰ˆ 0.82  
- **G-Eval:** mean â‰ˆ 8.31  
- **Correlation:** r â‰ˆ â€“0.33 (complementarity between metrics)

<p align="center">
  <img src="figures/bertscore_distribution.png" alt="BERTScore Distribution" width="70%"/>
</p>

<p align="center">
  <img src="figures/geval_distribution.png" alt="G-Eval Distribution" width="70%"/>
</p>


---

## ğŸ”§ Usage
Typical research use cases include:
1. **Review-conditioned edit planning**  
   Generate structured edit plans based on review feedback.  
2. **Evaluating automatic revisions**  
   Benchmark against human final versions using BERTScore + G-Eval.  
3. **Analyzing review facets**  
   Study how specific feedback drives textual changes.

---

## ğŸ“œ How to Load the Dataset
We provide a helper script [`drft_loader.py`](scripts/drft_loader.py) to make it easy to load the dataset into a pandas DataFrame.

### Example:
```python
from scripts.drft_loader import load_drft_dataset

# Load the dataset
df = load_drft_dataset("DRFT_dataset.xlsx")

# Explore
print(df.shape)
print(df.head())

---

## âš–ï¸ Ethical and Legal Considerations
- All materials are **publicly available** (ICLR/OpenReview, PeerRead, arXiv).  
- Reviewer anonymity is preserved.  
- Redistribution respects platform terms: where redistribution is restricted, stable IDs and retrieval scripts are provided.  
- **Intended use:** research on review-grounded rewriting, revision quality, and review uptake.  
- **Out of scope:** acceptance prediction, reviewer scoring, deanonymization, or private inference.  

---

## ğŸ“š Citation
If you use **DRFT** in your research, please cite:

```bibtex
@misc{carneiro2025drft,
  title     = {DRFT: A Corpus of Draft--Review--Final Triples for Modeling and Evaluating the Peer-Review Cycle},
  author    = {Leandro Dias Carneiro and FlÃ¡vio de Barros Vidal},
  year      = {2025},
  howpublished = {GitHub repository},
  url       = {https://github.com/ldiascarneiro/DRFT-dataset}
}
